# Recursos - Machine Learning Competitivo: top 1% de Kaggle 
Una recopilaciÃ³n de distintos recursos mencionados a lo largo de la charla del jueves 30 de septiembre de 2021.

**GrabaciÃ³n de la charla en YouTube**  
â–¶ï¸ https://www.youtube.com/watch?v=0o72ndnTnCQ&t=110s


## Modelos dominantes en datos estructurados 
### Deep Learning 

**TabNet : Attentive Interpretable Tabular Learning**  
ğŸ’» https://github.com/dreamquark-ai/tabnet  
ğŸ“‘ https://arxiv.org/abs/1908.07442  

**Tabular Data: Deep Learning is Not All You Need**  
ğŸ“‘ https://arxiv.org/abs/2106.03253

## TÃ©cnicas comunmente utilizadas 

### ValidaciÃ³n adversarial 

**Adversarial Validation: A Diagnostic Tool for Overfitting**  
ğŸ‘¨â€ğŸ« https://towardsdatascience.com/adversarial-validation-ca69303543cd

**How to use Adversarial Validation to Help Fix Overfitting**  
â–¶ï¸ https://www.youtube.com/watch?v=7cUCDRaIZ7I 


### Rapids 
**Rapids**  
ğŸ”— https://rapids.ai/ 

**cuML - GPU Machine Learning Algorithms**  
ğŸ’» https://github.com/rapidsai/cuml 

**10 Minutes to cuDF and Dask-cuDF**  
ğŸ‘¨â€ğŸ« https://docs.rapids.ai/api/cudf/stable/10min.html 

**Accelerating Trading on GPU via RAPIDS**  
ğŸ† https://www.kaggle.com/aerdem4/accelerating-trading-on-gpu-via-rapids 

**Your GPU Compute Capability**  
ğŸ”— https://developer.nvidia.com/cuda-gpus 

**How to Use t-SNE Effectively**  
ğŸ‘¨â€ğŸ« https://distill.pub/2016/misread-tsne/ 

**Understanding UMAP (Interactive comparison between UMAP and t-SNE)**  
ğŸ‘¨â€ğŸ« https://pair-code.github.io/understanding-umap/ 

**Basic Usage of HDBSCAN for Clustering**  
ğŸ‘¨â€ğŸ« https://hdbscan.readthedocs.io/en/latest/basic_hdbscan.html 


### SelecciÃ³n de features 

**LOFO (Leave One Feature Out) Importance**  
ğŸ’» https://github.com/aerdem4/lofo-importance

**Boruta-Shap**  
ğŸ’» https://github.com/Ekeany/Boruta-Shap 

**Consistent Feature Selection for Pattern Recognition in Polynomial Time**  
Analyze two different feature selection problems:
finding a minimal feature set optimal for classification (MINIMAL-OPTIMAL) vs. finding all features relevant to the target variable (ALL-RELEVANT).  
ğŸ“‘ https://dl.acm.org/doi/10.5555/1314498.1314519

### OptimizaciÃ³n de hiperparÃ¡metros 

**Optuna: A hyperparameter optimization framework**  
ğŸ’» https://github.com/optuna/optuna 

### Ensembling 

**1st Place Solution**, 30 days of ML  
By [HungNT](https://www.kaggle.com/hungkhoi)  
ğŸ‘¨â€ğŸ« https://www.kaggle.com/c/30-days-of-ml/discussion/269541






